```{r}
library(aod)
library(ggplot2)
mydata = read.csv("http://www.ats.ucla.edu/stat/data/binary.csv")
summary(mydata)
sapply(mydata, sd)
```

 two-way contingence table of categorical outcome and predictors we want to make sure there are not 0 cells

```{r}
xtabs(~admit + rank, data = mydata)
```

```{r}
mydata$rank = factor(mydata$rank)
mylogit = glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
```

we can use the confint function to obtain confidence intervals for the coefficient estimates. Note that for logistic models, CIs are based on the profiled log-likelihood function.

```{r}
confint(mylogit)
```
or we can get CIs based on just the standard errors by using the default method.
```{r}
confint.default(mylogit)
```

 we can test for an overall effect of rank using the wald.test function of the aod library. The order in which the coefficients are given in the table of coefficients is the same as the order of the terms in the model. This is important because the wald.test function refers to the coefficients by their ordee in the model. We use the wald.test function. b supplies the coefficients, while Sigma supplies the variance covariance matrix of the error terms, finally Terms tels R which terms in the model are to be tested, in this case, terms4,5,and 6, are the three terms for the levels of rank.

```{r}
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)
```

 The chi-squared test statistic of 20.9, with three degrees of freedom is associated with a p-value of 0.00011 indicating that the overall effect of rank is statistically significant.
 We can also test additional hypotheses about the differences in the coefficients for the different levels of rank. Below we test that the coefficient for rank=2 is equal to the coefficient for rank=3. The first line of code below creates a vector l that defines the test we want to perform. In this case, we want to test the difference (subtraction) of the terms for rank=2 and rank=3 (i.e., the 4th 
 and 5th terms in the model). To contrast these two terms, we multiply one of them by 1, and the other by -1. The other terms in the model are not involved in the test, so they are multiplied by 0. The second line of code below uses L=l to tell R that we wish to base the test on the vector l (rather than using the Terms option as we did above).

```{r}
l = cbind(0, 0, 0, 1, -1, 0)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), L= l)
```

 The chi-squared test statistic of 5.5 with 1 degree of freedom is associated with a p-value of 0.019, indicating that the difference between the coefficient for rank=2 and the coefficient for rank=3 is statistically significant.

```{r}
#odds ratios only
exp(coef(mylogit))
#odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
```

 Now we can say that for a one unit increase in gpa, the odds of being admitted to graduate school (versus not being admitted) increase by a factor of 2.23. 

 We can also use predicted probabilities.
```{r}
newdata1 = with(mydata, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))
newdata1$rankP = predict(mylogit, newdata = newdata1, type = "response")
newdata1
newdata2 = with(mydata, data.frame(gre = rep(seq(from = 200, to = 800, length.out = 100),4), gpa = mean(gpa), rank = factor(rep(1:4, each = 100))))
newdata3 = cbind(newdata2, predict(mylogit, newdata = newdata2, type = "link", se = TRUE))
newdata3 = within(newdata3, {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
})
head(newdata3)
ggplot(newdata3, aes(x = gre, y = PredictedProb)) + geom_ribbon(aes(ymin = LL,
    ymax = UL, fill = rank), alpha = 0.2) + geom_line(aes(colour = rank),
    size = 1)
```

